# -*- coding: utf-8 -*-
"""APS 2 - Analise de Sentimento - Davi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M9VBE6AVQEMWMlnqpkYVqOSvi2zHtT3Y

# Importando Bibliotecas
"""

import numpy as np
import pandas as pd
import nltk
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

# Baixar recursos adicionais do NLTK
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

"""# Carregando DataFrame"""

#https://www.kaggle.com/datasets/luisfredgs/imdb-ptbr?resource=download
df = pd.read_csv('/content/imdb-reviews-pt-br.csv')
df

"""# Análise Exploratória dos Dados"""

# Verificando o cabeçalho do dataset (primeiras 5 linhas)
df.head()

# Verificando o rodapé do dataset (últimas 5 linhas)
df.tail()

# Verificando o formato do dataset (número de linhas e colunas)
df.shape

# Verificando o tipo das features
df.dtypes

# Verificando se existem valores nulos
df.isnull().sum()

# Verificando se existem linhas duplicadas
df.duplicated().sum()

# Dropar a coluna 'text_en', pois contém as mesmas informações da coluna 'text_pt', porém em outro idioma
df = df.drop('text_en', axis=1)

# Verificando a porcentagem (%) dos valores da coluna 'Sentimento'
df['sentiment'].value_counts()/df.shape[0]

# Contando linhas por sentimento para ver o mapeamento
df.groupby(['sentiment']).count()

# Removendo pontuações
df['text_pt'] = df['text_pt'].str.translate(str.maketrans("", "", string.punctuation))

# Realizando tokenização
df['tokens'] = df['text_pt'].apply(word_tokenize)
df['tokens']

# Removendo stopwords - palavras que podem ser consideradas irrelevantes
stop_words = set(stopwords.words('portuguese'))
df['tokens'] = df['tokens'].apply(lambda tokens: [token for token in tokens if token.lower() not in stop_words])

# Realizando normalização de texto
lemmatizer = WordNetLemmatizer()
df['tokens'] = df['tokens'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])

# Realizando conversão para minúsculas
df['tokens'] = df['tokens'].apply(lambda tokens: [token.lower() for token in tokens])

# Resultado do pré-processamento
print(df['tokens'])

"""# Treinando o Modelo"""

# Extraindo recursos
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df['tokens'].apply(' '.join))  # Convertendo a lista de tokens em uma string
y = df['sentiment']

# Dividindo dados para treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Treinando o modelo em SVM com os dados de treino
svm = SVC()
svm.fit(X_train, y_train)

# Fazendo previsões nos dados de teste
y_pred = svm.predict(X_test)

"""# Avaliando o Modelo"""

cm = confusion_matrix(y_test, y_pred)
cm

# Calculando a Acurácia do Modelo
accuracy = accuracy_score(y_test, y_pred)
print('Acurácia: ',accuracy)

# Calculando a Precisão do Modelo
precision = precision_score(y_test, y_pred, average='macro')
print('Precisão: ',precision)

# Calculando o Recall do Modelo
recall = recall_score(y_test, y_pred, average='macro')
print('Recall: ',recall)

# Calculando o F1-Score do Modelo
f1 = f1_score(y_test, y_pred, average='macro')
print('F1-Score: ',f1)

"""# Conclusão

### Em resumo, após um período de aproximandamente 65min para o processamento do treinamento ser concluído, o modelo de análise de sentimentos em questão parece ter um desempenho satisfatório, com uma alta acurácia e F1-score, além de indicar um bom equilíbrio entre a precisão e o recall, o que significa que ele é capaz de classificar corretamente os sentimentos expressos nos textos em grande parte dos casos.
"""