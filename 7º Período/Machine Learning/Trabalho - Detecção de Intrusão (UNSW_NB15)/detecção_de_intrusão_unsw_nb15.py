# -*- coding: utf-8 -*-
"""Detecção de Intrusão - UNSW_NB15.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_xgwOhU-FRXGx0zEf1uyjZjqAz2p116s

# Importando Bibliotecas
"""

import pandas as pd
import numpy as np
import seaborn as sns
import time
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB

"""# Carregando DataFrame"""

# Os conjuntos de treinamento e teste foram invertidos, portanto, alteramos os nomes antes de carregá-los dos arquivos CSV.
train = pd.read_csv("/content/UNSW_NB15_training-set.csv")
test = pd.read_csv("/content/UNSW_NB15_testing-set.csv")
print("Tamanho do conjunto de treinamento: ", len(train))
print("Tamanho do conjunto de teste: ", len(test))

"""# Concatenando Datasets

### Para garantir o equilíbrio entre os conjuntos de treinamento e teste e evitar o processamento duplo, decidimos concatená-los em um dataframe e redividi-los com uma proporção diferente posteriormente com sklearn.model_selection.train_test_split().
"""

df = pd.concat([train, test])

# Tamanho em bytes
total_bytes = df.memory_usage().sum()

# Tamanho em gigabytes
total_gb = total_bytes / 1e9

print("Tamanho do novo conjunto em bytes: ", total_bytes)
print("Tamanho do novo conjunto em gigabytes: ", total_gb)

"""# Análise Exploratória dos Dados"""

# Verificando o cabeçalho do dataset (primeiras 5 linhas)
df.head()

# Verificando o rodapé do dataset (últimas 5 linhas)
df.tail()

# Verificando o formato do dataset (número de linhas e colunas)
df.shape

# Obtendo informações resumidas sobre o DataFrame
df.info()

# Descrição do dataframe
df.describe(include="all")

# Verificando se existem linhas duplicadas
df.duplicated().sum()

# Verificando se existem valores nulos
df.isnull().sum()

"""### Redução de dimensionalidade"""

# Descartando Features desnecessárias
df = df.drop(columns=['id', 'attack_cat'])
# id serve apenas para identificação
# Este é um problema de classificação binária, portanto, usamos apenas rótulo de coluna para classificar ataque (1) ou normal (0). Então, não precisamos dos detalhes do ataque em attack_cat.

# Codificação de recursos categóricos usando LabelEncoder
df_cat = df.select_dtypes(exclude=[np.number])
print(df_cat.columns)
for feature in df_cat.columns:
    df[feature] = LabelEncoder().fit_transform(df[feature])

df.head()
# 'proto', 'service', 'state'

# Identificando as colunas altamente correlacionadas
columns = df.columns.tolist()
corr = df.corr()
correlated_vars = []
for i in range(len(columns) - 1):
    for j in range(i+1, len(columns)):
        if corr[columns[i]][columns[j]] > 0.98: # verificando os valores de correlação entre as colunas maior que 0.98
            print(columns[i], columns[j], corr[columns[i]][columns[j]])
            correlated_vars.append(columns[j])

df.shape

# Essas colunas representam redundância ou não fornecem informações únicas para a análise, por isso vamos excluí-las
df = df.drop(columns=correlated_vars)

"""### Renomeando as colunas para facilitar a interpretação dos dados"""

df = df.rename(columns={'dur': 'duração', 'proto': 'protocolo', 'service': 'serviço', 'state': 'estado', 'spkts': 'cont. pacotes', 'dpkts': 'destino', 'sbytes': 'Bytes orig/dest', 'dbytes': 'destino bytes', 'rate': 'taxa transferência', 'sttl': 'tempo orig/dest', 'dttl': 'tempo dest/fonte', 'sload': 'Bits orig p/ segundo', 'dload': 'Bits dest p/ segundo', 'sloss': 'perda pacotes orig', 'dloss': 'perda pacotes dest', 'sinpkt': 'tempo IP orig', 'dinpkt': 'tempo IP dest', 'sjit': 'tremulação da fonte (mSec)', 'djit': 'tremulação de destino (mSec)', 'swin': 'janela TCP orig', 'stcpb': 'seq TCP orig', 'dtcpb': 'seq TCP dest', 'dwin': 'janela TCP dest', 'tcprtt': 'tempo ida/volta TCP', 'synack': 'tempo config TCP SYN/SYN_ACK', 'ackdat': 'tempo config TCP SYN_ACK/ACK', 'smean': 'média tam pacote src', 'dmean': 'média tam pacote dst', 'trans_depth': 'solic/resp http', 'response_body_len': 'tam real conteúdo', 'ct_srv_src': 'serv (14) orig (3)', 'ct_state_ttl': 'tempo de vida orig/dest', 'ct_dst_ltm': 'dest (3)', 'ct_src_dport_ltm': 'orig (1) dest (4)', 'ct_dst_sport_ltm': 'dest (3) orig (2)', 'ct_dst_src_ltm': 'orig (1) dest (3)', 'is_ftp_login': 'login user/senha', 'ct_ftp_cmd': 'fluxos ftp', 'ct_flw_http_mthd': 'fluxos com métodos http', 'ct_src_ltm': 'orig (1)', 'ct_srv_dst': 'serv (14) dest (3)', 'is_sm_ips_ports': 'IP orig/dest ou porta iguais', 'label': 'rótulo' })
df.head()

# Verificando a proporção entre o ataque e os dados normais
df['rótulo'].value_counts().plot.bar()

# Verificando a proporção entre o ataque e os dados normais
df['rótulo'].value_counts(normalize=True)

"""### A proporção entre o ataque e os dados normais não é igual, mas apenas ligeiramente desequilibrada. Portanto, não faremos uma correção de amostragem."""

# Análise de correlação entre features
corr = df.corr()
corr.style.background_gradient(cmap='PuBu')

# Calculando a matriz de correlação
matriz_correlacao = df.corr()

# Obtendo as correlações com a coluna rótulo
correlacoes_coluna_rotulo = matriz_correlacao['rótulo'].abs().sort_values(ascending=False)

# Exibindo as colunas com maior correlação
colunas_maior_correlacao = correlacoes_coluna_rotulo[1:6]  # Seleciona as 5 colunas com maior correlação (excluindo a própria coluna rótulo)

print("Colunas com maior correlação com a coluna rótulo:")
print(colunas_maior_correlacao)

"""### A coluna ‘tempo orig/dest’ refere-se ao tempo de vida da comunicação entre a origem e o destino em uma conexão de rede, e apesar dela apresentar uma correlação relativamente alta com a target, optamos por não removê-la. Isso pois, esse fator pode estar relacionado a outros problemas além da rede estar sob ataque ou não, como por exemplo, a velocidade da rede.

### Plotando os gráficos das colunas com maior correlação com a coluna 'Rótulo'
"""

# Criando o boxplot
boxplot = plt.boxplot(df['tempo orig/dest'])
plt.ylabel('Valores')
plt.xlabel('Valor ao vivo da origem ao destino')

# Obtendo os valores estatísticos do boxplot
valores_min = np.min(df['tempo orig/dest'])
valores_max = np.max(df['tempo orig/dest'])
q1 = np.percentile(df['tempo orig/dest'], 25)
mediana = np.median(df['tempo orig/dest'])
q3 = np.percentile(df['tempo orig/dest'], 75)

print('Mínimo:', valores_min)
print('Máximo:', valores_max)

print('Primeiro quartil (Q1):', q1)
print('Mediana (Q2):', mediana)
print('Terceiro quartil (Q3):', q3)

# Calculando os limites dos outliers
iqr = q3 - q1
limite_inferior = q1 - 1.5 * iqr
limite_superior = q3 + 1.5 * iqr

# Identificando os outliers
outliers = df[(df['tempo orig/dest'] < limite_inferior) | (df['tempo orig/dest'] > limite_superior)]

if len(outliers) > 0:
    print("Outliers encontrados:")
    unique_outliers = outliers.drop_duplicates(subset=['tempo orig/dest'])
    print(unique_outliers['tempo orig/dest'])
else:
    print("Nenhum outlier encontrado.")

plt.show()

# Criando o boxplot
boxplot = plt.boxplot(df['tempo de vida orig/dest'])
plt.ylabel('Valores')
plt.xlabel('Valores de tempo de vida origem/destino de cada estado em uma faixa')

# Obtendo os valores estatísticos do boxplot
valores_min = np.min(df['tempo de vida orig/dest'])
valores_max = np.max(df['tempo de vida orig/dest'])
q1 = np.percentile(df['tempo de vida orig/dest'], 25)
mediana = np.median(df['tempo de vida orig/dest'])
q3 = np.percentile(df['tempo de vida orig/dest'], 75)

print('Mínimo:', valores_min)
print('Máximo:', valores_max)

print('Primeiro quartil (Q1):', q1)
print('Mediana (Q2):', mediana)
print('Terceiro quartil (Q3):', q3)

# Calculando os limites dos outliers
iqr = q3 - q1
limite_inferior = q1 - 1.5 * iqr
limite_superior = q3 + 1.5 * iqr

# Identificando os outliers
outliers = df[(df['tempo de vida orig/dest'] < limite_inferior) | (df['tempo de vida orig/dest'] > limite_superior)]

if len(outliers) > 0:
    print("Outliers encontrados:")
    unique_outliers = outliers.drop_duplicates(subset=['tempo de vida orig/dest'])
    print(unique_outliers['tempo de vida orig/dest'])
else:
    print("Nenhum outlier encontrado.")

plt.show()

# Criando o boxplot
boxplot = plt.boxplot(df['estado'])
plt.ylabel('Valores')
plt.xlabel('Estado e seu protocolo dependente')

# Obtendo os valores estatísticos do boxplot
valores_min = np.min(df['estado'])
valores_max = np.max(df['estado'])
q1 = np.percentile(df['estado'], 25)
mediana = np.median(df['estado'])
q3 = np.percentile(df['estado'], 75)

print('Mínimo:', valores_min)
print('Máximo:', valores_max)

print('Primeiro quartil (Q1):', q1)
print('Mediana (Q2):', mediana)
print('Terceiro quartil (Q3):', q3)

# Calculando os limites dos outliers
iqr = q3 - q1
limite_inferior = q1 - 1.5 * iqr
limite_superior = q3 + 1.5 * iqr

# Identificando os outliers
outliers = df[(df['estado'] < limite_inferior) | (df['estado'] > limite_superior)]

if len(outliers) > 0:
    print("Outliers encontrados:")
    unique_outliers = outliers.drop_duplicates(subset=['estado'])
    print(unique_outliers['estado'])
else:
    print("Nenhum outlier encontrado.")

plt.show()

# Criando o boxplot
boxplot = plt.boxplot(df['dest (3) orig (2)'])
plt.ylabel('Valores')
plt.xlabel('Nº de conexões com destino (3) e porta de origem (2)')

# Obtendo os valores estatísticos do boxplot
valores_min = np.min(df['dest (3) orig (2)'])
valores_max = np.max(df['dest (3) orig (2)'])
q1 = np.percentile(df['dest (3) orig (2)'], 25)
mediana = np.median(df['dest (3) orig (2)'])
q3 = np.percentile(df['dest (3) orig (2)'], 75)

print('Mínimo:', valores_min)
print('Máximo:', valores_max)

print('Primeiro quartil (Q1):', q1)
print('Mediana (Q2):', mediana)
print('Terceiro quartil (Q3):', q3)

# Calculando os limites dos outliers
iqr = q3 - q1
limite_inferior = q1 - 1.5 * iqr
limite_superior = q3 + 1.5 * iqr

# Identificando os outliers
outliers = df[(df['dest (3) orig (2)'] < limite_inferior) | (df['dest (3) orig (2)'] > limite_superior)]

if len(outliers) > 0:
    print("Outliers encontrados:")
    unique_outliers = outliers.drop_duplicates(subset=['dest (3) orig (2)'])
    print(unique_outliers['dest (3) orig (2)'])
else:
    print("Nenhum outlier encontrado.")

plt.show()

# Criando o boxplot
boxplot = plt.boxplot(df['janela TCP orig'])
plt.ylabel('Valores')
plt.xlabel('Valor da janela TCP de origem')

# Obtendo os valores estatísticos do boxplot
valores_min = np.min(df['janela TCP orig'])
valores_max = np.max(df['janela TCP orig'])
q1 = np.percentile(df['janela TCP orig'], 25)
mediana = np.median(df['janela TCP orig'])
q3 = np.percentile(df['janela TCP orig'], 75)

print('Mínimo:', valores_min)
print('Máximo:', valores_max)

print('Primeiro quartil (Q1):', q1)
print('Mediana (Q2):', mediana)
print('Terceiro quartil (Q3):', q3)

# Calculando os limites dos outliers
iqr = q3 - q1
limite_inferior = q1 - 1.5 * iqr
limite_superior = q3 + 1.5 * iqr

# Identificando os outliers
outliers = df[(df['janela TCP orig'] < limite_inferior) | (df['janela TCP orig'] > limite_superior)]

if len(outliers) > 0:
    print("Outliers encontrados:")
    unique_outliers = outliers.drop_duplicates(subset=['janela TCP orig'])
    print(unique_outliers['janela TCP orig'])
else:
    print("Nenhum outlier encontrado.")

plt.show()

"""## Algumas possíveis explicações para a maior correlação observada entre as colunas

### 'tempo orig/dest' (Source Time to Live): Essa coluna pode estar relacionada à persistência dos pacotes enviados pela fonte. Esse é um caso muito comum em atividades maliciosas ou a determinados tipos de tráfego de rede.

### 'tempo de vida orig/dest' (Connection Tracking State Time to Live): Essa coluna indica o tempo de vida do estado de conexão. Ela pode estar associada a conexões maliciosas ou atividades de intrusão devido à sua relevância na análise de tráfego de rede e detecção de comportamentos anômalos.

### 'estado': Essa coluna indica o estado da conexão (por exemplo, 'FIN', 'SYN', 'RST', 'CON', etc.). O estado de uma conexão é uma informação importante para determinar se uma conexão é legítima ou suspeita.

### 'dest (3) orig (2)' (Count Destination Source Port Last Time): Essa coluna pode representar o número de vezes que um determinado par de porta de destino e porta de origem foi observado. Ao analisar isso, padrões incomuns de comunicação entre uma fonte e um destino específico podem ser identificados.

### 'janela TCP orig' (Source TCP Window Size): Essa coluna pode representar o tamanho da janela TCP na fonte. Pode estar associado a conexões maliciosas ou atividades de intrusão devido ao seu papel na comunicação de dados em uma conexão TCP.

# Treinando o Modelo
"""

# Dividindo dataframe entre X e y
X = df.drop(columns=['rótulo'])
feature_list = list(X.columns)
X = np.array(X)
y = df['rótulo']

# Dividindo dados para treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)

print("Dados para treino:", len(X_train))
print("Dados para teste:", len(X_test))

# Padronização dos dados usando o StandardScaler
scaler = StandardScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

# Definindo os modelos de classificação que serão utilizados e os armazenando em um dicionário
models = {}
models['Árvore de Decisão'] = DecisionTreeClassifier()
models['Random Forest'] = RandomForestClassifier()
models['Gaussian Naive Bayes'] = GaussianNB()

# Realizando o treinamento e avaliação de diferentes modelos de classificação
train_score, accuracy, precision, recall, f1, training_time, y_pred = {}, {}, {}, {}, {}, {}, {}
for key in models.keys():
    start_time = time.time() # Registrando o tempo de início do treinamento do modelo
    models[key].fit(X_train, y_train) # Realizando o treinamento do modelo específico associado à chave "key" usando os dados de treinamento (X_train) e os rótulos de treinamento (y_train).
    training_time[key] = time.time() - start_time

    y_pred[key] = models[key].predict(X_test) # Realizando a previsão do modelo específico nos dados de teste (X_test) e armazena as previsões no dicionário "y_pred"

    train_score[key] = models[key].score(X_train, y_train)
    accuracy[key] = models[key].score(X_test, y_test)
    precision[key] = precision_score(y_test, y_pred[key])
    recall[key] = recall_score(y_test, y_pred[key])
    f1[key] = f1_score(y_test, y_pred[key])

"""# Avaliação do Modelo"""

df_models = pd.DataFrame(index=models.keys(), columns=['Pontuação de Treinamento', 'Acurácia', 'Precisão', 'Recall', 'F1-Score', 'Tempo de Treinamento'])
df_models['Pontuação de Treinamento'] = train_score.values()
df_models['Acurácia'] = accuracy.values()
df_models['Precisão'] = precision.values()
df_models['Recall'] = recall.values()
df_models['F1-Score'] = f1.values()
df_models['Tempo de Treinamento'] = training_time.values()

df_models

"""### Com base nesses resultados, podemos concluir que o modelo Random Forest apresenta o melhor desempenho geral, com alta acurácia, precisão e F1-Score. No entanto, é importante considerar o tempo de treinamento, pois o Random Forest leva consideravelmente mais tempo para treinar. Se a velocidade de treinamento for um fator crítico, o modelo Gaussian Naive Bayes pode ser uma opção mais rápida, apesar de ter um desempenho um pouco inferior. A Árvore de Decisão também oferece um desempenho sólido, ficando entre os outros dois modelos em termos de métricas de desempenho e tempo de treinamento."""